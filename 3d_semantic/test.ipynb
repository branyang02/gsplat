{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, Literal, Tuple\n",
    "\n",
    "import tyro\n",
    "\n",
    "from datasets.clip import OpenCLIPNetwork, OpenCLIPNetworkConfig\n",
    "import nerfview\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import viser\n",
    "from utils import SAMOptModule\n",
    "from gsplat.rendering import rasterization\n",
    "\n",
    "from gsplat._helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_image(image_array: np.ndarray):\n",
    "    \"\"\"\n",
    "    Display an image from a numpy array.\n",
    "\n",
    "    Parameters:\n",
    "    image_array (numpy.ndarray): The image array to display. Expected shape is (height, width, channels).\n",
    "    \"\"\"\n",
    "    plt.imshow(image_array)\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"results/nerf_dff_dataset_depth/ckpts/ckpt_29999.pt\", map_location=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sam_module SAMOptModule(\n",
      "  (embeds): Embedding(49, 256)\n",
      "  (color_head): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      "  (feature_head): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=512, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def _get_sam_module(sam_state_dict):\n",
    "    n = sam_state_dict[\"embeds.weight\"].shape[0]\n",
    "    feature_dim = sam_state_dict[\"feature_head.4.weight\"].shape[0]\n",
    "    embed_dim = sam_state_dict[\"embeds.weight\"].shape[1]\n",
    "    mlp_width = sam_state_dict[\"color_head.0.weight\"].shape[0]\n",
    "    sh_degree = int(\n",
    "        math.sqrt(\n",
    "            sam_state_dict[\"color_head.0.weight\"].shape[1] - feature_dim - embed_dim\n",
    "        )\n",
    "        - 1\n",
    "    )\n",
    "    mlp_depth = len(sam_state_dict) // 2 - 4  ### TODO: double check this\n",
    "\n",
    "    sam_module = SAMOptModule(\n",
    "        n=n,\n",
    "        feature_dim=feature_dim,\n",
    "        embed_dim=embed_dim,\n",
    "        sh_degree=sh_degree,\n",
    "        mlp_width=mlp_width,\n",
    "        mlp_depth=mlp_depth,\n",
    "        output_dim=feature_dim,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    sam_module.load_state_dict(sam_state_dict)\n",
    "    print(\"sam_module\", sam_module)\n",
    "\n",
    "    return sam_module, sh_degree\n",
    "\n",
    "sam_module, sh_degree = _get_sam_module(ckpt[\"sam_module\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['colors', 'features', 'means3d', 'opacities', 'quats', 'scales'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splats = ckpt[\"splats\"]\n",
    "splats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenCLIPNetwork(\n",
       "  (model): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (patch_dropout): Identity()\n",
       "      (ln_pre): LayerNormFp32((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): ModuleList(\n",
       "          (0-11): 12 x ResidualAttentionBlock(\n",
       "            (ln_1): LayerNormFp32((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ls_1): Identity()\n",
       "            (ln_2): LayerNormFp32((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): GELU(approximate='none')\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ls_2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNormFp32((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (ln_final): LayerNormFp32((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip = OpenCLIPNetwork(OpenCLIPNetworkConfig)\n",
    "clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['camera_state', 'img_wh', 'kwargs'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('viewer_render_fn_inputs.pkl', 'rb') as f:\n",
    "    inputs = pickle.load(f)\n",
    "\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def viewer_render_fn(\n",
    "    camera_state: nerfview.CameraState, img_wh: Tuple[int, int], **kwargs\n",
    "):\n",
    "    feature_query = kwargs.get(\"feature_query\", None)\n",
    "    print(\"feature_query: \", feature_query)\n",
    "\n",
    "    tok_phrase = clip.tokenizer(feature_query).to(\"cuda\")\n",
    "    feature_embeds = clip.model.encode_text(tok_phrase)  # [1, 512]\n",
    "    feature_embeds /= feature_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    W, H = img_wh\n",
    "    c2w = camera_state.c2w\n",
    "    K = camera_state.get_K(img_wh)\n",
    "    c2w = torch.from_numpy(c2w).float().to(\"cuda\")\n",
    "    K = torch.from_numpy(K).float().to(\"cuda\")\n",
    "\n",
    "    render_colors, _, _, features, xyz = rasterize_splats(\n",
    "        camtoworlds=c2w[None],\n",
    "        Ks=K[None],\n",
    "        width=W,\n",
    "        height=H,\n",
    "        radius_clip=3.0,  # skip GSs that have small image radius (in gt_colors)\n",
    "        feature_embeds=feature_embeds,\n",
    "        # segment=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "    colors = render_colors[..., :3]  # [1, H, W, 3]\n",
    "    render_features = render_colors[..., 3:]  # [1, H, W, 512]\n",
    "\n",
    "    feature_embeds = feature_embeds.view(1, 1, 1, 512)\n",
    "    cosine_similarity = F.cosine_similarity(render_features, feature_embeds, dim=-1)\n",
    "\n",
    "    threshold = kwargs.get(\"feature_similarity_threshold\", 0.3)\n",
    "\n",
    "    mask = cosine_similarity > threshold\n",
    "\n",
    "    new_color = torch.tensor([1.0, 0.0, 0.0], device=colors.device)\n",
    "    colors[mask] = new_color\n",
    "\n",
    "    xyz = xyz[..., :3]\n",
    "\n",
    "\n",
    "    return colors[0].cpu().numpy(), features, xyz[0].cpu().numpy()\n",
    "\n",
    "def rasterize_splats(\n",
    "        camtoworlds: Tensor,\n",
    "        Ks: Tensor,\n",
    "        width: int,\n",
    "        height: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Tensor, Tensor, Dict]:\n",
    "        means = splats[\"means3d\"]\n",
    "        print(\"NUM OF GAUSSIANS: \", means.shape[0])\n",
    "        quats = splats[\"quats\"]\n",
    "        scales = torch.exp(splats[\"scales\"])\n",
    "        opacities = torch.sigmoid(splats[\"opacities\"])\n",
    "\n",
    "        image_ids = kwargs.pop(\"image_ids\", None)\n",
    "\n",
    "        # get colors from sam_module\n",
    "        colors, features = sam_module(\n",
    "            features=splats[\"features\"],\n",
    "            embed_ids=image_ids,\n",
    "            dirs=means[None, :, :] - camtoworlds[:, None, :3, 3],\n",
    "            sh_degree=sh_degree,\n",
    "        )\n",
    "        colors = colors + splats[\"colors\"]\n",
    "        colors = torch.sigmoid(colors)\n",
    "\n",
    "        features = features + splats[\"features\"]\n",
    "        colors_with_features = torch.cat([colors, features], dim=-1)\n",
    "\n",
    "        render_colors, render_alphas, info = rasterization(\n",
    "            means=means,\n",
    "            quats=quats,\n",
    "            scales=scales,\n",
    "            opacities=opacities,\n",
    "            colors=colors_with_features,\n",
    "            viewmats=torch.linalg.inv(camtoworlds),  # [C, 4, 4]\n",
    "            Ks=Ks,  # [C, 3, 3]\n",
    "            width=width,\n",
    "            height=height,\n",
    "            packed=False,\n",
    "            absgrad=False,\n",
    "            sparse_grad=False,\n",
    "            rasterize_mode=\"classic\",\n",
    "            sh_degree=None,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        rendered_xyz, _, _ = rasterization(\n",
    "            means=means,\n",
    "            quats=quats,\n",
    "            scales=scales,\n",
    "            opacities=opacities,\n",
    "            colors=means,\n",
    "            viewmats=torch.linalg.inv(camtoworlds),  # [C, 4, 4]\n",
    "            Ks=Ks,  # [C, 3, 3]\n",
    "            width=width,\n",
    "            height=height,\n",
    "            packed=False,\n",
    "            absgrad=False,\n",
    "            sparse_grad=False,\n",
    "            rasterize_mode=\"classic\",\n",
    "            sh_degree=None,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # xyz_flattened = rendered_xyz.view(-1, 3)\n",
    "        # distances = torch.cdist(xyz_flattened, means)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return render_colors, render_alphas, info, features, rendered_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m( ●    )\u001b[0m \u001b[1;33mgsplat: Setting up CUDA with MAX_JOBS=10 (This may take a few minutes \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m(  ●   )\u001b[0m \u001b[1;33mgsplat: Setting up CUDA with MAX_JOBS=10 (This may take a few minutes \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m(  ●   )\u001b[0m \u001b[1;33mgsplat: Setting up CUDA with MAX_JOBS=10 (This may take a few minutes \u001b[0m\n",
      "\u001b[1;33mthe first time)\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2Kfeature_query:  carrot\n",
      "NUM OF GAUSSIANS:  386490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of gsplat.cuda._backend failed: Traceback (most recent call last):\n",
      "  File \"/sfs/weka/scratch/jqm9ba/Repos/gsplat/gsplat/cuda/_backend.py\", line 55, in <module>\n",
      "    from gsplat import csrc as _C\n",
      "ImportError: cannot import name 'csrc' from 'gsplat' (/sfs/weka/scratch/jqm9ba/Repos/gsplat/gsplat/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/sfs/weka/scratch/jqm9ba/Repos/gsplat/gsplat/cuda/_backend.py\", line 101, in <module>\n",
      "    _C = load(\n",
      "  File \"/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1309, in load\n",
      "    return _jit_compile(\n",
      "  File \"/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1719, in _jit_compile\n",
      "    _write_ninja_file_and_build_library(\n",
      "  File \"/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1809, in _write_ninja_file_and_build_library\n",
      "    extra_ldflags = _prepare_ldflags(\n",
      "OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n",
      "]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fully_fused_projection_fwd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcarrot\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m render_image, features, xyz \u001b[39m=\u001b[39m viewer_render_fn(camera_state\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mcamera_state\u001b[39;49m\u001b[39m\"\u001b[39;49m], img_wh\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mimg_wh\u001b[39;49m\u001b[39m\"\u001b[39;49m], feature_query\u001b[39m=\u001b[39;49mquery, feature_similarity_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m)\n",
      "File \u001b[0;32m/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mviewer_render_fn\u001b[0;34m(camera_state, img_wh, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m c2w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(c2w)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m K \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(K)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m render_colors, _, _, features, xyz \u001b[39m=\u001b[39m rasterize_splats(\n\u001b[1;32m     19\u001b[0m     camtoworlds\u001b[39m=\u001b[39;49mc2w[\u001b[39mNone\u001b[39;49;00m],\n\u001b[1;32m     20\u001b[0m     Ks\u001b[39m=\u001b[39;49mK[\u001b[39mNone\u001b[39;49;00m],\n\u001b[1;32m     21\u001b[0m     width\u001b[39m=\u001b[39;49mW,\n\u001b[1;32m     22\u001b[0m     height\u001b[39m=\u001b[39;49mH,\n\u001b[1;32m     23\u001b[0m     radius_clip\u001b[39m=\u001b[39;49m\u001b[39m3.0\u001b[39;49m,  \u001b[39m# skip GSs that have small image radius (in gt_colors)\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m     feature_embeds\u001b[39m=\u001b[39;49mfeature_embeds,\n\u001b[1;32m     25\u001b[0m     \u001b[39m# segment=True,\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m colors \u001b[39m=\u001b[39m render_colors[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m3\u001b[39m]  \u001b[39m# [1, H, W, 3]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m render_features \u001b[39m=\u001b[39m render_colors[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m:]  \u001b[39m# [1, H, W, 512]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m, in \u001b[0;36mrasterize_splats\u001b[0;34m(camtoworlds, Ks, width, height, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m features \u001b[39m=\u001b[39m features \u001b[39m+\u001b[39m splats[\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     72\u001b[0m colors_with_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([colors, features], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m render_colors, render_alphas, info \u001b[39m=\u001b[39m rasterization(\n\u001b[1;32m     75\u001b[0m     means\u001b[39m=\u001b[39;49mmeans,\n\u001b[1;32m     76\u001b[0m     quats\u001b[39m=\u001b[39;49mquats,\n\u001b[1;32m     77\u001b[0m     scales\u001b[39m=\u001b[39;49mscales,\n\u001b[1;32m     78\u001b[0m     opacities\u001b[39m=\u001b[39;49mopacities,\n\u001b[1;32m     79\u001b[0m     colors\u001b[39m=\u001b[39;49mcolors_with_features,\n\u001b[1;32m     80\u001b[0m     viewmats\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(camtoworlds),  \u001b[39m# [C, 4, 4]\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m     Ks\u001b[39m=\u001b[39;49mKs,  \u001b[39m# [C, 3, 3]\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m     width\u001b[39m=\u001b[39;49mwidth,\n\u001b[1;32m     83\u001b[0m     height\u001b[39m=\u001b[39;49mheight,\n\u001b[1;32m     84\u001b[0m     packed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     85\u001b[0m     absgrad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     86\u001b[0m     sparse_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     87\u001b[0m     rasterize_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclassic\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     88\u001b[0m     sh_degree\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     89\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     90\u001b[0m )\n\u001b[1;32m     92\u001b[0m rendered_xyz, _, _ \u001b[39m=\u001b[39m rasterization(\n\u001b[1;32m     93\u001b[0m     means\u001b[39m=\u001b[39mmeans,\n\u001b[1;32m     94\u001b[0m     quats\u001b[39m=\u001b[39mquats,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[39m# xyz_flattened = rendered_xyz.view(-1, 3)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# distances = torch.cdist(xyz_flattened, means)\u001b[39;00m\n",
      "File \u001b[0;32m/sfs/weka/scratch/jqm9ba/Repos/gsplat/gsplat/rendering.py:219\u001b[0m, in \u001b[0;36mrasterization\u001b[0;34m(means, quats, scales, opacities, colors, viewmats, Ks, width, height, near_plane, far_plane, radius_clip, eps2d, sh_degree, packed, tile_size, backgrounds, render_mode, sparse_grad, absgrad, rasterize_mode, channel_chunk, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[39massert\u001b[39;00m (sh_degree \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], colors\u001b[39m.\u001b[39mshape\n\u001b[1;32m    218\u001b[0m \u001b[39m# Project Gaussians to 2D. Directly pass in {quats, scales} is faster than precomputing covars.\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m proj_results \u001b[39m=\u001b[39m fully_fused_projection(\n\u001b[1;32m    220\u001b[0m     means,\n\u001b[1;32m    221\u001b[0m     \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# covars,\u001b[39;49;00m\n\u001b[1;32m    222\u001b[0m     quats,\n\u001b[1;32m    223\u001b[0m     scales,\n\u001b[1;32m    224\u001b[0m     viewmats,\n\u001b[1;32m    225\u001b[0m     Ks,\n\u001b[1;32m    226\u001b[0m     width,\n\u001b[1;32m    227\u001b[0m     height,\n\u001b[1;32m    228\u001b[0m     eps2d\u001b[39m=\u001b[39;49meps2d,\n\u001b[1;32m    229\u001b[0m     packed\u001b[39m=\u001b[39;49mpacked,\n\u001b[1;32m    230\u001b[0m     near_plane\u001b[39m=\u001b[39;49mnear_plane,\n\u001b[1;32m    231\u001b[0m     far_plane\u001b[39m=\u001b[39;49mfar_plane,\n\u001b[1;32m    232\u001b[0m     radius_clip\u001b[39m=\u001b[39;49mradius_clip,\n\u001b[1;32m    233\u001b[0m     sparse_grad\u001b[39m=\u001b[39;49msparse_grad,\n\u001b[1;32m    234\u001b[0m     calc_compensations\u001b[39m=\u001b[39;49m(rasterize_mode \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mantialiased\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    235\u001b[0m )\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m packed:\n\u001b[1;32m    238\u001b[0m     \u001b[39m# The results are packed into shape [nnz, ...]. All elements are valid.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     (\n\u001b[1;32m    240\u001b[0m         camera_ids,\n\u001b[1;32m    241\u001b[0m         gaussian_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m         compensations,\n\u001b[1;32m    247\u001b[0m     ) \u001b[39m=\u001b[39m proj_results\n",
      "File \u001b[0;32m/sfs/weka/scratch/jqm9ba/Repos/gsplat/gsplat/cuda/_wrapper.py:260\u001b[0m, in \u001b[0;36mfully_fused_projection\u001b[0;34m(means, covars, quats, scales, viewmats, Ks, width, height, eps2d, near_plane, far_plane, radius_clip, packed, sparse_grad, calc_compensations)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m _FullyFusedProjectionPacked\u001b[39m.\u001b[39mapply(\n\u001b[1;32m    244\u001b[0m         means,\n\u001b[1;32m    245\u001b[0m         covars,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m         calc_compensations,\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    259\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[39mreturn\u001b[39;00m _FullyFusedProjection\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    261\u001b[0m         means,\n\u001b[1;32m    262\u001b[0m         covars,\n\u001b[1;32m    263\u001b[0m         quats,\n\u001b[1;32m    264\u001b[0m         scales,\n\u001b[1;32m    265\u001b[0m         viewmats,\n\u001b[1;32m    266\u001b[0m         Ks,\n\u001b[1;32m    267\u001b[0m         width,\n\u001b[1;32m    268\u001b[0m         height,\n\u001b[1;32m    269\u001b[0m         eps2d,\n\u001b[1;32m    270\u001b[0m         near_plane,\n\u001b[1;32m    271\u001b[0m         far_plane,\n\u001b[1;32m    272\u001b[0m         radius_clip,\n\u001b[1;32m    273\u001b[0m         calc_compensations,\n\u001b[1;32m    274\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/jqm9ba/envs/gsplat-cuda/lib/python3.9/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/sfs/weka/scratch/jqm9ba/Repos/gsplat/gsplat/cuda/_wrapper.py:712\u001b[0m, in \u001b[0;36m_FullyFusedProjection.forward\u001b[0;34m(ctx, means, covars, quats, scales, viewmats, Ks, width, height, eps2d, near_plane, far_plane, radius_clip, calc_compensations)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    695\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    696\u001b[0m     ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n\u001b[1;32m    711\u001b[0m     \u001b[39m# \"covars\" and {\"quats\", \"scales\"} are mutually exclusive\u001b[39;00m\n\u001b[0;32m--> 712\u001b[0m     radii, means2d, depths, conics, compensations \u001b[39m=\u001b[39m _make_lazy_cuda_func(\n\u001b[1;32m    713\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mfully_fused_projection_fwd\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    714\u001b[0m     )(\n\u001b[1;32m    715\u001b[0m         means,\n\u001b[1;32m    716\u001b[0m         covars,\n\u001b[1;32m    717\u001b[0m         quats,\n\u001b[1;32m    718\u001b[0m         scales,\n\u001b[1;32m    719\u001b[0m         viewmats,\n\u001b[1;32m    720\u001b[0m         Ks,\n\u001b[1;32m    721\u001b[0m         width,\n\u001b[1;32m    722\u001b[0m         height,\n\u001b[1;32m    723\u001b[0m         eps2d,\n\u001b[1;32m    724\u001b[0m         near_plane,\n\u001b[1;32m    725\u001b[0m         far_plane,\n\u001b[1;32m    726\u001b[0m         radius_clip,\n\u001b[1;32m    727\u001b[0m         calc_compensations,\n\u001b[1;32m    728\u001b[0m     )\n\u001b[1;32m    729\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m calc_compensations:\n\u001b[1;32m    730\u001b[0m         compensations \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/sfs/weka/scratch/jqm9ba/Repos/gsplat/gsplat/cuda/_wrapper.py:12\u001b[0m, in \u001b[0;36m_make_lazy_cuda_func.<locals>.call_cuda\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_cuda\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m      9\u001b[0m     \u001b[39m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_backend\u001b[39;00m \u001b[39mimport\u001b[39;00m _C\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(_C, name)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fully_fused_projection_fwd'"
     ]
    }
   ],
   "source": [
    "query = \"carrot\"\n",
    "render_image, features, xyz = viewer_render_fn(camera_state=inputs[\"camera_state\"], img_wh=inputs[\"img_wh\"], feature_query=query, feature_similarity_threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'render_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display_image(render_image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'render_image' is not defined"
     ]
    }
   ],
   "source": [
    "display_image(render_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xyz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display_image(xyz)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xyz' is not defined"
     ]
    }
   ],
   "source": [
    "display_image(xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsplat-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
